\subsection{Hosting Database on a Cloud Server}
Once the soccer data has been preprocessed into a set of tables in *.csv format, the database will be created and hosted on a cloud-based platform. For our project, Heroku platform was chosen to host our soccer database, and potentially, a web UI for viewing and adding data.
Based on its documentation, \begin{quote}
    'Heroku is a platform as a service based on a managed container system, with integrated data services and a powerful ecosystem, for deploying and running modern apps.' \cite{b10}
\end{quote} Basically, a Heroku app server is the center element of a web-hosted project, with additional on-demand services such as data services, continuous integration, and security services attached to the app. In our case, we are using a Heroku app with Heroku Postgresql as an Add-on. The advantage of using Heroku platform is mainly the convenience. Since the soccer data is not particularly large, we don't need to spend extra effort to host the database elsewhere and configure it from scratch. Heroku provides a web-based dashboard to monitor different services where we can access and view details of the Postgresql database with just a few clicks. The Postgresql add-on even provided a data-clip function that the database owner can run SQL queries and export the returned result as a *.csv file.
To host the database on Heroku Postgresql, we need to first install the Heroku Cli and log in through the terminal. The detailed install instructions can be found in the official Heroku documentation \cite{b11}. The Heroku app server can be started by simply running the command at the root of the project repository: 
\begin{verbatim}
    heroku create
    git push heroku main
\end{verbatim}
The next step is to add the Heroku Postgresql to the new app. Adding a Heroku Postgresql is quite simple to search and add the add-on using the add-on search bar inside the 'Resource' tab of the app dashboard. Once selecting the corresponding plans, a PostgreSQL is created on the app server and is not online ad accessible. One way of creating and adding tables is through the Heroku Postgresql Cli. Start the psql tool interface at the terminal by calling: \begin{verbatim}
    heroku pg:psql
\end{verbatim} 
Then we can run the CREATE SQL command for each existing tables. Using league table as an example, the CREATE command is:
\begin{verbatim}
    CREATE TABLE league
    (
        league_id integer NOT NULL,
        league_name character varying(50) COLLATE pg_catalog."default",
        CONSTRAINT league_pkey PRIMARY KEY (league_id)
    )
\end{verbatim}
Alternatively, we can connect the database to PgAdmin to create the tables. The database credentials are located under the settings table of the database dashboard, and the detailed instructions to connect to PgAdmin are shown in the next section. 
Once we have put in the schema of all the tables, we can use the COPY command to move the pre-processed data to the database:
\begin{verbatim}
    \copy league 
    FROM \your\table\location\league.csv WITH (FORMAT CSV, DELIMITER ',', HEADER true);
\end{verbatim}
After migrating all data to the Heroku Pstgresql database, the database setup is complete and ready to use.
%- Brief introduction and Installing Heroku
%    -Heroku Cli
%    -Heroku Postgresql
%- Adding tables to the database using the heroku postgres cli
\subsection{Using The Database}
The owner of the app 
% - Connecting to PG Admin 
% - Querying data and export
%     - use sample query as an example
\section{Discussion}
    % - what worked
    %     - preprocessing was well organized and didn't need to be changed much
    %     - extract enough info to solve various meaningful problems
    %     - stable hosting strategy with good postgresql cloud server
    %         - heroku worked, versus AWS didn't
Due to the degree of data organization achieved during the pre-processing stage, not much needed to be adjusted for the final database. For example, much of the data sourced from the Kaggle data set was already organized in a way that aligned with the original intentions of the database. Additionally, the data was mostly normalized when the database was initially drafted, so not much time had to be delegated to normalize the data during the later stages of the project. Having a normalized database early in the project timeline likely prevented any functionality issues while constructing queries for the data and helped delegate time to other aspects of the project later in the project timeline. Further, the dimensionality of the data was able to lead to meaningful conclusions about the data set, such as how the number of games was affected by the COVID-19 pandemic, calculating match points for each game and season, and which play had the highest game point average. 

    % - what didn't work
    %     + encoding issues
    %     + initial matching of players to nationality issues because of UTF
    %     + matched coaches to teams manually
    %     - couldn't connect to AWS service with access issues
    %     - loading data into database through heroku (Jean)
    %     - getting set up for UI (Jean)

To add a level of complexity to the data and more details about each Player, additional information was collected from outside sources to generate each player's nationality and year they were born, and add managers to the database. %cite. 
This data was collected and temporarily held in a playerDetails table for processing. The playerDetails table consisted of a player's name, three-character country code, and year born. Before combining the Players and playerDetails tables, duplicate player names were removed from the playerDetails table, and an additional table %cite 
was used to match the country code in playerDetails with the full name of the country for data presentation reasons. The player name in playerDetails was used as the joining key to merge the playerDetails with the Players table. Many of the player names in the Players table included special characters that were not UTF-8, which caused some encoding issues when merging the two tables. There were many instances when one data set would spell a player's name with or without an accent or other special character while the other table wouldn't, so when the data was merged in Pandas an exact match for a player's name wasn't always found in the playerDetails table. This led to many Players without a nationality or year born, even though certain players did exist in the playerDetails table but with different encoding which wasn't recognized as an exact match. To remedy this, both data sets were converted to Latin-1 in Pandas before merging the two tables and the special characters were manually replaced within Excel with a UTF-8 version of the character. For example, the character '\'e' was replaced with an 'e', using Excel to replace all features. This adjustment in the data did find matches for a larger portion of players, but since the data was found from two different sources, there were still many players in the Players table that did not exist in the playerDetails table. To maintain continuity in the database, the remaining players that didn't match a nationality or year born were auto-generated using a random value from the countries already existing in the playerDetails table. Another issue encountered during pre-processing was collecting and adding Manager information to the database. When it was originally decided to add this information to the database, it was decided to manually source all the team managers for the database's time period and add it to the database. Since there were only about 150 teams, the task didn't appear to be too complex or time-consuming. Further, considering little existing knowledge on data mining amongst the team, it seemed that developing any other way to collect this data would be more costly, out of the scope of the project, and not conducive to the project timeline. What was not anticipated was the frequency and complexity of team manager changes which led to this being a much more laborious task than expected. In retrospect, it would have been beneficial to develop another, more efficient way to collect this information in the database. While deciding what platform would be best to host the database, access issues were encountered when connecting to a server with Amazon Web Services (AWS).