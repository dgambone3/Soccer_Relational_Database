\subsection{Hosting Database on a Cloud Server}
Once the soccer data has been preprocessed into a set of tables in *.csv format, the database will be created and hosted on a cloud-based platform. For our project, Heroku platform was chosen to host our soccer database, and potentially, a web UI for viewing and adding data.
Based on its documentation, \begin{quote}
    'Heroku is a platform as a service based on a managed container system, with integrated data services and a powerful ecosystem, for deploying and running modern apps.' \cite{b10}
\end{quote} Basically, a Heroku app server is the center element of a web-hosted project, with additional on-demand services such as data services, continuous integration, and security services attached to the app. In our case, we are using a Heroku app with Heroku Postgresql as an Add-on. The advantage of using Heroku platform is mainly the convenience. Since the soccer data is not particularly large, we don't need to spend extra effort to host the database elsewhere and configure it from scratch. Heroku provides a web-based dashboard to monitor different services where we can access and view details of the Postgresql database with just a few clicks. The Postgresql add-on even provided a data-clip function that the database owner can run SQL queries and export the returned result as a *.csv file.
To host the database on Heroku Postgresql, we need to first install the Heroku Cli and log in through the terminal. The detailed install instructions can be found in the official Heroku documentation \cite{b11}. The Heroku app server can be started by simply running the command at the root of the project repository: 
\begin{verbatim}
    heroku create
    git push heroku main
\end{verbatim}
The next step is to add the Heroku Postgresql to the new app. Adding a Heroku Postgresql is quite simple to search and add the add-on using the add-on search bar inside the 'Resource' tab of the app dashboard. Once selecting the corresponding plans, a PostgreSQL is created on the app server and is not online ad accessible. One way of creating and adding tables is through the Heroku Postgresql Cli. Start the psql tool interface at the terminal by calling: \begin{verbatim}
    heroku pg:psql
\end{verbatim} 
Then we can run the CREATE SQL command for each existing tables. Using league table as an example, the CREATE command is:
\begin{verbatim}
    CREATE TABLE league
    (
        league_id integer NOT NULL,
        league_name character varying(50) COLLATE pg_catalog."default",
        CONSTRAINT league_pkey PRIMARY KEY (league_id)
    )
\end{verbatim}
Alternatively, we can connect the database to PgAdmin to create the tables. The database credentials are located under the settings table of the database dashboard, and the detailed instructions to connect to PgAdmin are shown in the next section. 
Once we have put in the schema of all the tables, we can use the COPY command to move the pre-processed data to the database:
\begin{verbatim}
    \copy league 
    FROM \your\table\location\league.csv WITH (FORMAT CSV, DELIMITER ',', HEADER true);
\end{verbatim}
After migrating all data to the Heroku Pstgresql database, the database setup is complete and ready to use.
%- Brief introduction and Installing Heroku
%    -Heroku Cli
%    -Heroku Postgresql
%- Adding tables to the database using the heroku postgres cli
\subsection{Using The Database}
The owner of the Heroku app can directly access and query the database as highlighted in the previous section, see Figure \ref{dataclip}. Other users can also query the soccer database by connecting via PgAdmin. The following information is essential and can be found in the 'Settings' tab of the database web dashboard:
\begin{itemize}
    Host
    Database Name
    User
    Port
\end{itemize}
The corresponding location to fill into PgAdmin 'register database' wizard is shown in Figure \ref{link}. Additionally, we need to restrain the visibility of our database by adding the database name to the restrictions, see Figure \ref{advance}.
% - Connecting to PG Admin 
% - Querying data and export
%     - use sample query as an example
\section{Discussion}
    % - what worked
    %     - preprocessing was well organized and didn't need to be changed much
    %     - extract enough info to solve various meaningful problems
    %     - stable hosting strategy with good postgresql cloud server
    %         - heroku worked, versus AWS didn't
Due to the degree of data organization achieved during the pre-processing stage, not much needed to be adjusted for the final database. For example, much of the data sourced from the Kaggle data set was already organized in a way that aligned with the original intentions of the database. Additionally, the data was mostly normalized when the database was initially drafted, so not much time had to be delegated to normalize the data during the later stages of the project. Having a normalized database early in the project timeline likely prevented any functionality issues while constructing queries for the data and helped delegate time to other aspects of the project later in the project timeline. Further, the dimensionality of the data was able to lead to meaningful conclusions about the data set, such as how the number of games was affected by the COVID-19 pandemic, calculating match points for each game and season, and which play had the highest game point average. 

    % - what didn't work
    %     + encoding issues
    %     + initial matching of players to nationality issues because of UTF
    %     + matched coaches to teams manually
    %     - couldn't connect to AWS service with access issues
    %     - loading data into database through heroku (Jean)
    %     - getting set up for UI (Jean)

To add a level of complexity to the data and more details about each Player, additional information was collected from outside sources to generate each player's nationality and year they were born, and add managers to the database. %cite. 
This data was collected and temporarily held in a playerDetails table for processing. The playerDetails table consisted of a player's name, three-character country code, and year born. Before combining the Players and playerDetails tables, duplicate player names were removed from the playerDetails table, and an additional table %cite 
was used to match the country code in playerDetails with the full name of the country for data presentation reasons. The player name in playerDetails was used as the joining key to merge the playerDetails with the Players table. Many of the player names in the Players table included special characters that were not UTF-8, which caused some encoding issues when merging the two tables. There were many instances when one data set would spell a player's name with or without an accent or other special character while the other table wouldn't, so when the data was merged in Pandas an exact match for a player's name wasn't always found in the playerDetails table. This led to many Players without a nationality or year born, even though certain players did exist in the playerDetails table but with different encoding which wasn't recognized as an exact match. To remedy this, both data sets were converted to Latin-1 in Pandas before merging the two tables and the special characters were manually replaced within Excel with a UTF-8 version of the character. For example, the character '\'e' was replaced with an 'e', using Excel to replace all features. This adjustment in the data did find matches for a larger portion of players, but since the data was found from two different sources, there were still many players in the Players table that did not exist in the playerDetails table. To maintain continuity in the database, the remaining players that didn't match a nationality or year born were auto-generated using a random value from the countries already existing in the playerDetails table. Another issue encountered during pre-processing was collecting and adding Manager information to the database. When it was originally decided to add this information to the database, it was decided to manually source all the team managers for the database's period and add it to the database. Since there were only about 150 teams, the task didn't appear to be too complex or time-consuming. Further, considering little existing knowledge on data mining amongst the team, it seemed that developing any other way to collect this data would be more costly, out of the scope of the project, and not conducive to the project timeline. What was not anticipated was the frequency and complexity of team manager changes which led to this being a much more laborious task than expected. In retrospect, it would have been beneficial to develop another, more efficient way to collect this information in the database. While deciding what platform would be best to host the database, access issues were encountered when connecting to a server with Amazon Web Services (AWS).

The most surprising fact of the project was how many issues would arise solely from the actual data, and not how the data was structured or organized. For example, adding Player nationality and year born didn't seem to be nearly as troublesome of a task as it ended up being. It was unforeseen that adding this information would be as finicky as it was, specifically with encountering multiple UTF-8 errors. These errors were present multiple times not only while merging playerDetails with the Players table but also while loading the data into the database through Heroku and PG Admin. Another surprising issue was encountered during the final stages of project development where it was realized that a Player could not be traced to a specific Team at any given time. This was mostly due to the nature of the football industry, in which it is very common for players to be traded, or switch teams multiple times throughout a season. Thus, even finding a team roster for a specific season would not be sufficient information since data for this aspect of the sport is not static. Roster information would have to be collected for each team for each game, or each team and the duration each player had played for each team, which would have amounted to thousands of data points. In the initial stages of this project, it was not recognized that the database wouldn't be able to report this information and was an unexpected discovery encountered while drafting queries for the database. As a seemingly essential and principal data point that would be very common information for a user to analyze, this issue proved to be a fracture in the database. Given this deficiency was found during the late stages of project development and the approaching deadline, it was decided to leave the data as is, and not provide this information in the database. Particularly considering the number of issues uncovered while adding two attributes to the Players table, it was certain that attempting to add this data at a such late stage in the project would be more problematic than its worth. If this issue was recognized in the initial stages of the project, the database may have been constructed differently and with additional information so the database would be able to join each Player to their respective Team. The level of complexity needed to connect a Season to a Player, or Season to a Team was another unexpected reality of the database. To join either of these two tables, it would be necessary to join the Games table as well. This was recognized as a trade-off between the normalization of the data, and the database's ease of use. Though this was not a large issue and didn't largely affect the database's practicality, it is something that could have been addressed in the earlier stages of project development and possibly avoided if the data was structured differently.
Another unanticipated realization during project development was how complex it would be to present Team rankings for a Season based on Game points. The database holds information for each Seasons champion, but an interesting and commonly used statistic of the sport is to rank the teams based on match points acquired throughout the Season. It was decided that this information would be beneficial to analyze, thus this data was chosen as an example query for the database. Though this data was available in the database, the query proved to be more complicated than anticipated based on the amount of aggregation needed to generate the rankings.